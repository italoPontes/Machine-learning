{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade de Regressão Linear\n",
    "\n",
    "## Código-fonte disponível em: [link](https://github.com/italoPontes/Machine-learning/tree/master/Tarefas/Regress%C3%A3o-Linear-Simples-do-Zero)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#Federal University of Campina Grande (UFCG)\n",
    "#Author: Ítalo de Pontes Oliveira\n",
    "#Adapted from: Siraj Raval\n",
    "#Available at: https://github.com/llSourcell/linear_regression_live\n",
    "\n",
    "#The optimal values of m and b can be actually calculated with way less effort than doing a linear regression. \n",
    "#this is just to demonstrate gradient descent\n",
    "\n",
    "\"\"\"This project will calculate linear regression\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy\n",
    "from numpy import *\n",
    "import sys\n",
    "\n",
    "## Calculate a new linear and angular coefficient step by a learning rate. \n",
    "#  @param w0_current Current linear coefficient\n",
    "#  @param w1_current Current linear coefficient\n",
    "#  @param x Domain points\n",
    "#  @param y Image points\n",
    "#  @param learningRate The rate in which the gradient will be changed in one step\n",
    "def step_gradient(w0_current, w1_current, x, y, learningRate):\n",
    "\tw0_gradient = 0\n",
    "\tw1_gradient = 0\n",
    "\tnorma = 0\n",
    "\tN = float(len(x))\n",
    "\t\n",
    "\tw0_gradient = -2 * sum( y - ( w0_current + ( w1_current * x ) ) ) / N\n",
    "\tw1_gradient = -2 * sum( ( y - ( w0_current + ( w1_current * x ) ) ) * x ) / N\n",
    "\n",
    "\tnorma = numpy.linalg.norm(w0_gradient - w1_gradient)\n",
    "\t\n",
    "\tnew_w0 = w0_current - (learningRate * w0_gradient)\n",
    "\tnew_w1 = w1_current - (learningRate * w1_gradient)\n",
    "\t\n",
    "\treturn [new_w0, new_w1, norma]\n",
    "\n",
    "## Run the descending gradient\n",
    "#  @param x Domain points\n",
    "#  @param y Image points\n",
    "#  @param starting_w0 Linear coefficient initial\n",
    "#  @param starting_w1 Angular coefficient initial\n",
    "#  @param learning_rate The rate in which the gradient will be changed in one step\n",
    "#  @param num_iterations Interactions number that the slope line will approximate before a stop.\n",
    "#  @param output_filename Figure name to save iteraction x RSS graphic\n",
    "def gradient_descent_runner(x, y, starting_w0, starting_w1, learning_rate, num_iterations, output_filename):\n",
    "\tw0 = starting_w0\n",
    "\tw1 = starting_w1\n",
    "\trss_by_step = 0\n",
    "\trss_total = []\n",
    "\tnorma = learning_rate\n",
    "\ti = 0\n",
    "\t\n",
    "\tcondiction = True\n",
    "\tif num_iteractions < 1:\n",
    "\t\tcondiction = False\n",
    "\t\n",
    "\twhile (norma > (learning_rate/1000) and not condiction) or ( i < num_iteractions and condiction):\n",
    "\t\tw0, w1, norma = step_gradient(w0, w1, x, y, learning_rate)\n",
    "\t\t\n",
    "\t\trss_by_step = compute_error_for_line_given_points(w0, w1, x, y)\n",
    "\t\trss_total.append(rss_by_step)\n",
    "\t\ti += 1\n",
    "\n",
    "\tsave_figure(rss_total, \"Iteraction\", \"RSS\", output_filename)\t\n",
    "\t\n",
    "\treturn [w0, w1, i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Questões\n",
    "\n",
    "\n",
    "#### 1. Rode o mesmo programa nos dados contendo anos de escolaridade (primeira coluna) versus salário (segunda coluna). Baixe os dados aqui. Esse exemplo foi trabalhado em sala de aula em várias ocasiões. Os itens a seguir devem ser respondidos usando esses dados.\n",
    "\n",
    "RESOLUÇÃO: Arquivo baixado, encontra-se no diretório atual com o nome \"income.csv\".\n",
    "\n",
    "#### 2. Modifique o código original para imprimir o RSS a cada iteração do gradiente descendente.\n",
    "\n",
    "RESOLUÇÃO: Foi preferível adicionar uma nova funcionalidade ao código. Ao final da execução é salvo um gráfico com o RSS para todas as iterações.\n",
    "\n",
    "#### 3. O que acontece com o RSS ao longo das iterações (aumenta ou diminui) se você usar 1000 iterações e um learning_rate (tamanho do passo do gradiente) de 0.001? Por que você acha que isso acontece?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
