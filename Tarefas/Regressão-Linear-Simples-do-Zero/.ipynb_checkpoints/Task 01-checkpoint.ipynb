{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade de Regressão Linear\n",
    "\n",
    "## Código-fonte disponível em: [link](https://github.com/italoPontes/Machine-learning/tree/master/Tarefas/Regress%C3%A3o-Linear-Simples-do-Zero)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#Federal University of Campina Grande (UFCG)\n",
    "#Author: Ítalo de Pontes Oliveira\n",
    "#Adapted from: Siraj Raval\n",
    "#Available at: https://github.com/llSourcell/linear_regression_live\n",
    "\n",
    "#The optimal values of m and b can be actually calculated with way less effort than doing a linear regression. \n",
    "#this is just to demonstrate gradient descent\n",
    "\n",
    "\"\"\"This project will calculate linear regression\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy\n",
    "from numpy import *\n",
    "import sys\n",
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "## Compute the errors for a given line\n",
    "#  @param b Is the linear coefficient\n",
    "#  @param m Is the angular coefficient\n",
    "#  @param x Domain points\n",
    "#  @param y Domain points\n",
    "def compute_error_for_line_given_points(w0, w1, x, y):\n",
    "\ttotalError = sum((y - (w1 * x + w0)) ** 2)\n",
    "\ttotalError /= float(len(x))\n",
    "\treturn totalError\n",
    "\n",
    "## Calculate a new linear and angular coefficient step by a learning rate. \n",
    "#  @param w0_current Current linear coefficient\n",
    "#  @param w1_current Current linear coefficient\n",
    "#  @param x Domain points\n",
    "#  @param y Image points\n",
    "#  @param learningRate The rate in which the gradient will be changed in one step\n",
    "def step_gradient(w0_current, w1_current, x, y, learningRate):\n",
    "\tw0_gradient = 0\n",
    "\tw1_gradient = 0\n",
    "\tnorma = 0\n",
    "\tN = float(len(x))\n",
    "\t\n",
    "\tw0_gradient = -2 * sum( y - ( w0_current + ( w1_current * x ) ) ) / N\n",
    "\tw1_gradient = -2 * sum( ( y - ( w0_current + ( w1_current * x ) ) ) * x ) / N\n",
    "\n",
    "\tnorma = numpy.linalg.norm(w0_gradient - w1_gradient)\n",
    "\t\n",
    "\tnew_w0 = w0_current - (learningRate * w0_gradient)\n",
    "\tnew_w1 = w1_current - (learningRate * w1_gradient)\n",
    "\t\n",
    "\treturn [new_w0, new_w1, norma]\n",
    "\n",
    "## Run the descending gradient\n",
    "#  @param x Domain points\n",
    "#  @param y Image points\n",
    "#  @param starting_w0 Linear coefficient initial\n",
    "#  @param starting_w1 Angular coefficient initial\n",
    "#  @param learning_rate The rate in which the gradient will be changed in one step\n",
    "#  @param num_iterations Interactions number that the slope line will approximate before a stop.\n",
    "def gradient_descent_runner(x, y, starting_w0, starting_w1, learning_rate, num_iterations):\n",
    "\tw0 = starting_w0\n",
    "\tw1 = starting_w1\n",
    "\trss_by_step = 0\n",
    "\trss_total = []\n",
    "\tnorma = learning_rate\n",
    "\titeration_number = 0\n",
    "\t\n",
    "\tcondiction = True\n",
    "\tif num_iterations < 1:\n",
    "\t\tcondiction = False\n",
    "    \n",
    "\twhile (norma > 0.001 and not condiction) or ( iteration_number < num_iterations and condiction):\n",
    "\t\trss_by_step = compute_error_for_line_given_points(w0, w1, x, y)\n",
    "\t\trss_total.append(rss_by_step)\n",
    "\t\tw0, w1, norma = step_gradient(w0, w1, x, y, learning_rate)\n",
    "\t\titeration_number += 1\n",
    "\t\n",
    "\treturn [w0, w1, iteration_number, rss_total]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Questões\n",
    "\n",
    "\n",
    "#### 1. Rode o mesmo programa nos dados contendo anos de escolaridade (primeira coluna) versus salário (segunda coluna). Baixe os dados aqui. Esse exemplo foi trabalhado em sala de aula em várias ocasiões. Os itens a seguir devem ser respondidos usando esses dados.\n",
    "\n",
    "RESOLUÇÃO: Arquivo baixado, encontra-se no diretório atual com o nome \"income.csv\".\n",
    "\n",
    "#### 2. Modifique o código original para imprimir o RSS a cada iteração do gradiente descendente.\n",
    "\n",
    "RESOLUÇÃO: Foi preferível adicionar uma nova funcionalidade ao código. Ao final da execução é salvo um gráfico com o RSS para todas as iterações.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Show figure \n",
    "#  @param data Data to show in the graphic.\n",
    "#  @param xlabel Text to be shown in abscissa axis.\n",
    "#  @param ylabel Text to be shown in ordinate axis.\n",
    "def show_figure(data, xlabel, ylabel):\n",
    "\tplt.plot(data)\n",
    "\tplt.xlabel(xlabel)\n",
    "\tplt.ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3. O que acontece com o RSS ao longo das iterações (aumenta ou diminui) se você usar 1000 iterações e um learning_rate (tamanho do passo do gradiente) de 0.001? Por que você acha que isso acontece?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS na última iteração: 104.12\n",
      "RSS na última iteração: 117.13\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ0ocIiKSEyUOERHJyf8HT9bjxKZk7YIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f534ebd9490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "points = genfromtxt(\"income.csv\", delimiter=\",\")\n",
    "x = points[:,0] \n",
    "y = points[:,1]\n",
    "\n",
    "starting_w0 = 0\n",
    "starting_w1 = 0\n",
    "\n",
    "learning_rate = 0.001\n",
    "iterations_number = 50\n",
    "[w0, w1, iter_number, rss_total] = gradient_descent_runner(x, y, starting_w0, starting_w1, learning_rate, iterations_number)\n",
    "show_figure(rss_total, \"Iteraction\", \"RSS\") \n",
    "print(\"RSS na última iteração: %.2f\" % rss_total[-1])\n",
    "\n",
    "learning_rate = 0.0001\n",
    "[w0, w1, iter_number, rss_total] = gradient_descent_runner(x, y, starting_w0, starting_w1, learning_rate, iterations_number)\n",
    "show_figure(rss_total, \"Iteraction\", \"RSS\") \n",
    "print(\"RSS na última iteração: %.2f\" % rss_total[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Com esse gráfico é possível observar que:\n",
    "#### Quanto maior o Learning Rate, maior o número de iterações necessárias para se atingir um mesmo erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS na última iteração: 91.83\n",
      "RSS na última iteração: 41.02\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "iterations_number = 1000\n",
    "[w0, w1, iter_number, rss_total] = gradient_descent_runner(x, y, starting_w0, starting_w1, learning_rate, iterations_number)\n",
    "print(\"RSS na última iteração: %.2f\" % rss_total[-1])\n",
    "\n",
    "iterations_number = 10000\n",
    "[w0, w1, iter_number, rss_total] = gradient_descent_runner(x, y, starting_w0, starting_w1, learning_rate, iterations_number)\n",
    "print(\"RSS na última iteração: %.2f\" % rss_total[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao observar os valores de RSS calculados quando o número de iterações aumenta, é possível observar que o RSS obtido diminui cada vez mais.\n",
    "\n",
    "#### 4. Teste valores diferentes do número de iterações e learning_rate até que w0 e w1 sejam aproximadamente iguais a -39 e 5 respectivamente. Reporte os valores do número de iterações e learning_rate usados para atingir esses valores.\n",
    "\n",
    "Foram testados diferentes valores para o número de iterações, e diferentes frações do Learning Rate até que com a seguinte configuração, obteve o valor desejado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0: -39.11\n",
      "W1: 5.58\n",
      "RSS na última iteração: 29.83\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0025\n",
    "iterations_number = 20000\n",
    "[w0, w1, iter_number, rss_total] = gradient_descent_runner(x, y, starting_w0, starting_w1, learning_rate, iterations_number)\n",
    "\n",
    "print(\"W0: %.2f\" % w0)\n",
    "print(\"W1: %.2f\" % w1)\n",
    "print(\"RSS na última iteração: %.2f\" % rss_total[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. O algoritmo do vídeo usa o número de iterações como critério de parada. Mude o algoritmo para considerar um critério de tolerância que é comparado ao tamanho do gradiente (como no algoritmo dos slides apresentados em sala). \n",
    "\n",
    "A metodologia aplicada foi a seguinte: quando não se fornece o número de iterações por parâmetro, o algoritmo irá iterar até que a norma do gradiente descendente seja igual a 0,001. Ou:\n",
    "\n",
    "$\\vert\\vert (W_{0}^{grad}, W_{1}^{grad} ) \\vert\\vert < 0,001 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0: -39.44\n",
      "W1: 5.60\n",
      "RSS na última iteração: 29.83\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0025\n",
    "iterations_number = 0\n",
    "[w0, w1, iter_number, rss_total] = gradient_descent_runner(x, y, starting_w0, starting_w1, learning_rate, iterations_number)\n",
    "\n",
    "print(\"W0: %.2f\" % w0)\n",
    "print(\"W1: %.2f\" % w1)\n",
    "print(\"RSS na última iteração: %.2f\" % rss_total[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Ache um valor de tolerância que se aproxime dos valores dos parâmetros do item 4 acima. Que valor foi esse?\n",
    "\n",
    "O valor utilizado, conforme descrito na questão anterior, foi 0,001. Ou seja, quando o tamanho do gradiente for menor que 0,001, então, o algoritmo entenderá que a aproximação convergiu e terminará o processamento.\n",
    "\n",
    "#### 7. Implemente a forma fechada (equações normais) de calcular os coeficientes de regressão (vide algoritmo nos slides). Compare o tempo de processamento com o gradiente descendente considerando sua solução do item 6.\n",
    "\n",
    "Foi implementada a função considerando a forma fechada. Dessa maneira, foi observado que o tempo de processamento descrito na questão 6 foi, aproximadamente, cinco vezes maior. Mesmo considerando o código implementado já na versão vetorizada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo para calcular os coeficientes pelo gradiente descendente: 1.90 s.\n",
      "Tempo para calcular os coeficientes de maneira fechada: 0.0002 s.\n",
      "Ou seja, calcular os coeficientes por meio da forma fechada é 8003 vezes mais rápido que via gradiente.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "[w0, w1, iter_number, rss_total] = gradient_descent_runner(x, y, starting_w0, starting_w1, learning_rate, iterations_number)\n",
    "gradient_time = float(time.time()-start_time)\n",
    "print(\"Tempo para calcular os coeficientes pelo gradiente descendente: %.2f s.\" % gradient_time)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "## Compute the W0 and W1 by derivative\n",
    "#  @param x Domain points\n",
    "#  @param y Image points\n",
    "def compute_normal_equation(x, y):\n",
    "\tx_mean = numpy.mean(x)\n",
    "\ty_mean = numpy.mean(y)\n",
    "\tw1 = sum((x - x_mean)*(y - y_mean))/sum((x - x_mean)**2)\n",
    "\tw0 = y_mean-(w1*x_mean)\n",
    "\treturn [w0, w1]\n",
    "\n",
    "derivative_time = float(time.time()-start_time)\n",
    "print(\"Tempo para calcular os coeficientes de maneira fechada: %.4f s.\" % derivative_time)\n",
    "\n",
    "ratio = float(gradient_time/derivative_time)\n",
    "print(\"Ou seja, calcular os coeficientes por meio da forma fechada é %.0f vezes mais rápido que via gradiente.\" % (ratio))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
